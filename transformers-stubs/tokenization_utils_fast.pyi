from typing import Any, Dict, List, Optional, Tuple, Union

from .convert_slow_tokenizer import convert_slow_tokenizer
from .file_utils import add_end_docstrings
from .tokenization_utils import PreTrainedTokenizer
from .tokenization_utils_base import (
    PreTrainedTokenizerBase,
)

class PreTrainedTokenizerFast(PreTrainedTokenizerBase):

    slow_tokenizer_class: PreTrainedTokenizer = None
    @property
    def is_fast(self) -> bool: ...
    @property
    def vocab_size(self) -> int: ...
    def get_vocab(self) -> Dict[str, int]: ...
    def get_added_vocab(self) -> Dict[str, int]: ...
    def __len__(self) -> int: ...
    @overload
    def convert_tokens_to_ids(self, tokens: str) -> int: ...
    @overload
    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]: ...
    def num_special_tokens_to_add(self, pair: bool = False) -> int: ...
    @overload
    def convert_ids_to_tokens(
        self, ids: List[int], skip_special_tokens: bool = False
    ) -> List[str]: ...
    @overload
    def convert_ids_to_tokens(
        self, ids: int, skip_special_tokens: bool = False
    ) -> str: ...
    def tokenize(
        self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False
    ) -> List[str]: ...
