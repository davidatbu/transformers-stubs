# flake8: noqa
import abc
from transformers.tokenization_utils_base import (
    PreTrainedTokenizerBase as PreTrainedTokenizerBase,
)
from typing import Any, List, Literal, Mapping, Optional, Sequence, Tuple, Union

class InputExample:
    guid: Optional[str]
    text_a: str
    text_b: Optional[str] = ...
    label: Optional[str] = ...
    def to_json_string(self) -> str: ...
    # def __init__(self, guid: Any, text_a: Any, text_b: Any, label: Any) -> None: ...

class InputFeatures:
    input_ids: List[int]
    attention_mask: Optional[List[int]] = ...
    token_type_ids: Optional[List[int]] = ...
    label: Optional[Union[int, float]] = ...
    def to_json_string(self) -> str: ...
    # def __init__(
    # self, input_ids: Any, attention_mask: Any, token_type_ids: Any, label: Any
    # ) -> None: ...

class DataProcessor(abc.ABC, metaclass=abc.ABCMeta):
    # @abc.abstractmethod
    # def get_example_from_tensor_dict(
    # self, tensor_dict: Mapping[Any, Any]
    # ) -> InputExample: ...
    @abc.abstractmethod
    def get_train_examples(self, data_dir: str) -> List[InputExample]: ...
    @abc.abstractmethod
    def get_dev_examples(self, data_dir: str) -> List[InputExample]: ...
    def get_test_examples(self, data_dir: str) -> List[InputExample]: ...
    @abc.abstractmethod
    def get_labels(self) -> List[str]: ...
